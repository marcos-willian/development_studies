x1 = c(1,2,3)
x2 = c(3,4,5)
x1 - x2
x3 = rbind(x1,x2)
x3
dim(x3)
#Variaveis gerais
cores = c("red", "blue", "green")
#Criando 1000 amostras de treinamento
espiral<-mlbench.spirals(1000,1,0.05)
#Os dados de treinamento gerados plotados
plot(espiralTreinamento$x, col = cores[espiralTreinamento$classes],
xlab = "X", ylab = "Y", main = "Dados de treinamento")
rm(list = ls())
source("bayesClassificator.R")
library(caTools) #Função sample.split
library("mlbench")
library("plot3D")
#Variaveis gerais
cores = c("red", "blue", "green")
#Criando 1000 amostras de treinamento
espiral<-mlbench.spirals(1000,1,0.05)
#Os dados de treinamento gerados plotados
plot(espiralTreinamento$x, col = cores[espiralTreinamento$classes],
xlab = "X", ylab = "Y", main = "Dados de treinamento")
#Os dados de treinamento gerados plotados
plot(espiral$x, col = cores[espiral$classes],
xlab = "X", ylab = "Y", main = "Dados de treinamento")
#Treinamento modelo
bayesTreino = bayesC(espiral$x, espiral$classes)
source("kdeClassificator.R")
#Treinamento modelo
bayesTreino = bayesC(espiral$x, espiral$classes)
#Treinamento modelo
kdeTreino = bayesC(espiral$x, espiral$classes)
#Plota superficie de decisão
n = 100
xGrid = seq(from = -1, to = 1, length = n)
grid = expand.grid(X1 = xGrid, X2 = xGrid)
decisionSurface = matrix(as.numeric(kdeTreino$classificar(kdeTreino, grid)), n, n)
plot(espiralTreinamento$x, col = cores[espiralTreinamento$classes],
xlab = "X", ylab = "Y", main = "Contorno de separação das classes")
contour(xGrid, xGrid, decisionSurface, add = TRUE, drawlabels = FALSE)
#Plota superficie de decisão
n = 10
xGrid = seq(from = -1, to = 1, length = n)
grid = expand.grid(X1 = xGrid, X2 = xGrid)
decisionSurface = matrix(as.numeric(kdeTreino$classificar(kdeTreino, grid)), n, n)
plot(espiralTreinamento$x, col = cores[espiralTreinamento$classes],
xlab = "X", ylab = "Y", main = "Contorno de separação das classes")
contour(xGrid, xGrid, decisionSurface, add = TRUE, drawlabels = FALSE)
#Plota superficie de decisão
n = 10
xGrid = seq(from = -1, to = 1, length = n)
grid = expand.grid(X1 = xGrid, X2 = xGrid)
decisionSurface = matrix(as.numeric(kdeTreino$classificar(kdeTreino, grid)), n, n)
exp(xGrid)
#Plota superficie de decisão
n = 10
xGrid = seq(from = -1, to = 1, length = n)
grid = expand.grid(X1 = xGrid, X2 = xGrid)
grid = cbind(grid$X1,grid$X2)
decisionSurface = matrix(as.numeric(kdeTreino$classificar(kdeTreino, grid)), nrow = n, ncol = n)
plot(espiralTreinamento$x, col = cores[espiralTreinamento$classes],
xlab = "X", ylab = "Y", main = "Contorno de separação das classes")
plot(espiral$x, col = cores[espiral$classes],
xlab = "X", ylab = "Y", main = "Contorno de separação das classes")
contour(xGrid, xGrid, decisionSurface, add = TRUE, drawlabels = FALSE)
rm(list = ls())
source("kdeClassificator.R")
library(caTools) #Função sample.split
library("mlbench")
library("plot3D")
#Variaveis gerais
cores = c("red", "blue", "green")
#Criando 1000 amostras de treinamento
espiral<-mlbench.spirals(1000,1,0.05)
#Os dados de treinamento gerados plotados
plot(espiral$x, col = cores[espiral$classes],
xlab = "X", ylab = "Y", main = "Dados de treinamento")
#Treinamento modelo
kdeTreino = bayesC(espiral$x, espiral$classes, 0.02)
#Plota superficie de decisão
n = 10
xGrid = seq(from = -1, to = 1, length = n)
grid = expand.grid(X1 = xGrid, X2 = xGrid)
grid = cbind(grid$X1,grid$X2)
decisionSurface = matrix(as.numeric(kdeTreino$classificar(kdeTreino, grid)), nrow = n, ncol = n)
plot(espiral$x, col = cores[espiral$classes],
xlab = "X", ylab = "Y", main = "Contorno de separação das classes")
contour(xGrid, xGrid, decisionSurface, add = TRUE, drawlabels = FALSE)
rm(list = ls())
source("kdeClassificator.R")
library(caTools) #Função sample.split
library("mlbench")
library("plot3D")
#Variaveis gerais
cores = c("red", "blue", "green")
#Criando 1000 amostras de treinamento
espiral<-mlbench.spirals(1000,1,0.05)
#Os dados de treinamento gerados plotados
plot(espiral$x, col = cores[espiral$classes],
xlab = "X", ylab = "Y", main = "Dados de treinamento")
#Treinamento modelo
kdeTreino = bayesC(espiral$x, espiral$classes, 0.02)
#Plota superficie de decisão
n = 100
xGrid = seq(from = -1, to = 1, length = n)
grid = expand.grid(X1 = xGrid, X2 = xGrid)
grid = cbind(grid$X1,grid$X2)
decisionSurface = matrix(as.numeric(kdeTreino$classificar(kdeTreino, grid)), nrow = n, ncol = n)
plot(espiral$x, col = cores[espiral$classes],
xlab = "X", ylab = "Y", main = "Contorno de separação das classes")
contour(xGrid, xGrid, decisionSurface, add = TRUE, drawlabels = FALSE)
kde <- function(point,data,h){
N <- nrow(data)
prob <- 0
for (i in 1:N) {
dist <- 0
for (j in 1:ncol(data)) {
dist <- dist + (point[j] - data[i, j])^2
}
prob <- prob + exp(-dist/(2*(h^2)))
}
return(prob/(N*((sqrt(2*pi)*h)^2)))
}
rm(list = ls())
kde <- function(point,data,h){
N <- nrow(data)
prob <- 0
for (i in 1:N) {
dist <- 0
for (j in 1:ncol(data)) {
dist <- dist + (point[j] - data[i, j])^2
}
prob <- prob + exp(-dist/(2*(h^2)))
}
return(prob/(N*((sqrt(2*pi)*h)^2)))
}
rm(list=ls())
library('mlbench')
library('plot3D')
spiral<-mlbench.spirals(1000,cycles=1, sd=0.05)
plot(spiral)
data <- spiral$x
classes <- matrix(as.numeric(spiral$classes), ncol = 1)
seqi <- seq(-1, 1, 0.05)
seqj <- seq(-1, 1, 0.05)
M_sep <- matrix(0, nrow=length(seqi), ncol=length(seqj))
M_sup1 <- matrix(0, nrow=length(seqi), ncol=length(seqj))
M_sup2 <- matrix(0, nrow=length(seqi), ncol=length(seqj))
accur <- seq(0, 0, length.out = 10)
max_accur <- 0
for (i in 1:10) {
train_index <- sample(1:nrow(data), 0.7 * nrow(data))
test_index <- setdiff(1:nrow(data), train_index)
data_train <- data[train_index, 1:2]
classes_train <- classes[train_index, 1]
data_test <- data[test_index, 1:2]
classes_test <- classes[test_index, 1]
N1 <- length(classes_train[classes_train == 1])
N2 <- length(classes_train[classes_train == 2])
h <- 1.06 * sd(data_train)*((nrow(data_train))^(-1/5))
nAcertos <- 0
nErros <- 0
for (j in 1:nrow(data_test)) {
class1prob <- kde(data_test[j, ], data_train[classes_train == 1, 1:ncol(data)], h)*N1
class2prob <- kde(data_test[j, ], data_train[classes_train == 2, 1:ncol(data)], h)*N2
if (class1prob/class2prob > 1) guessedClass <- 1
else guessedClass <- 2
if (guessedClass == classes_test[j]) nAcertos <- nAcertos + 1
else nErros <- nErros + 1
}
accur[i] = nAcertos / (nAcertos + nErros)
if (accur[i] > max_accur) {
max_accur <- accur[i]
cx<-0
for (x in seqi){
cx<-cx+1
cj<-0
for (j in seqj){
cj<-cj+1
M_sup1[cx, cj] <- kde(c(x, j), data_train[classes_train == 1, 1:ncol(data)], h)*N1
M_sup2[cx, cj] <- kde(c(x, j), data_train[classes_train == 2, 1:ncol(data)], h)*N2
if (M_sup1[cx, cj]/M_sup2[cx, cj] > 1) M_sep[cx,cj] <- 1
else M_sep[cx,cj] <- 2
}
}
data_train_final <- data_train
}
}
for (j in 1:nrow(data_test)) {
class1prob <- kde(data_test[j, ], data_train[classes_train == 1, 1:ncol(data)], h)*N1
class2prob <- kde(data_test[j, ], data_train[classes_train == 2, 1:ncol(data)], h)*N2
if (classes_test[j] == 1) color <- 'red'
else color <- 'black'
if (j != 1) par(new=T)
plot(class1prob/(N1+N2), class2prob/(N1+N2), col=color,
xlim=c(0,0.6), ylim=c(0,0.6), xlab='', ylab='')
}
par(new=T)
plot(c(0, 0.6), c(0, 0.6), 'l',
xlim=c(0,0.6), ylim=c(0,0.6), xlab='', ylab='')
plot(spiral)
par(new=T)
contour2D(M_sep, seqi, seqj, xlim = c(-1.04, 0.8), ylim = c(-0.96, 0.965), xlab = '', ylab = '')
persp3D(seqi, seqj, M_sup1, counter=T, theta=55, phi=75, r=40, d=0.1, expand=0.5,
ltheta=90, lphi=180, shade=0.4, ticktype="detailed", nticks=5)
persp3D(seqi, seqj, M_sup2, counter=T, theta=55, phi=75, r=40, d=0.1, expand=0.5,
ltheta=90, lphi=180, shade=0.4, ticktype="detailed", nticks=5, add=T)
rm(list = ls())
source("kdeClassificator.R")
library(caTools) #Função sample.split
library("mlbench")
library("plot3D")
#Variaveis gerais
cores = c("red", "blue", "green")
#Criando 1000 amostras de treinamento
espiral<-mlbench.spirals(1000,1,0.05)
#Os dados de treinamento gerados plotados
plot(espiral$x, col = cores[espiral$classes],
xlab = "X", ylab = "Y", main = "Dados de treinamento")
#Treinamento modelo
kdeTreino = bayesC(espiral$x, espiral$classes, 0.02)
#Plota superficie de decisão
n = 100
xGrid = seq(from = -1, to = 1, length = n)
grid = expand.grid(X1 = xGrid, X2 = xGrid)
grid = cbind(grid$X1,grid$X2)
decisionSurface = matrix(as.numeric(kdeTreino$classificar(kdeTreino, grid)), nrow = n, ncol = n)
rm(list = ls())
source("kdeClassificator.R")
library(caTools) #Função sample.split
library("mlbench")
library("plot3D")
#Variaveis gerais
cores = c("red", "blue", "green")
#Criando 1000 amostras de treinamento
espiral<-mlbench.spirals(1000,1,0.05)
#Os dados de treinamento gerados plotados
plot(espiral$x, col = cores[espiral$classes],
xlab = "X", ylab = "Y", main = "Dados de treinamento")
#Treinamento modelo
kdeTreino = bayesC(espiral$x, espiral$classes, 0.02)
#Treinamento modelo
kdeTreino = bayesC(espiral$x, espiral$classes)
source("kdeClassificator.R")
#Treinamento modelo
kdeTreino = bayesC(espiral$x, espiral$classes)
#Plota superficie de decisão
n = 100
xGrid = seq(from = -1, to = 1, length = n)
grid = expand.grid(X1 = xGrid, X2 = xGrid)
grid = cbind(grid$X1,grid$X2)
decisionSurface = matrix(as.numeric(kdeTreino$classificar(kdeTreino, grid)), nrow = n, ncol = n)
a = matrix()
a[1,1] = 2
a[1,2] = 3
a = matrix(ncol = 2)
a[1,2] = 3
a[1,1] = 2
source("kdeClassificator.R")
#Treinamento modelo
kdeTreino = bayesC(espiral$x, espiral$classes)
decisionSurface = matrix(as.numeric(kdeTreino$classificar(kdeTreino, grid)), nrow = n, ncol = n)
rm(list = ls())
source("kdeClassificator.R")
library(caTools) #Função sample.split
library("mlbench")
library("plot3D")
#Variaveis gerais
cores = c("red", "blue", "green")
#Criando 1000 amostras de treinamento
espiral<-mlbench.spirals(1000,1,0.05)
#Os dados de treinamento gerados plotados
plot(espiral$x, col = cores[espiral$classes],
xlab = "X", ylab = "Y", main = "Dados de treinamento")
#Treinamento modelo
kdeTreino = bayesC(espiral$x, espiral$classes)
#Plota superficie de decisão
n = 100
xGrid = seq(from = -1, to = 1, length = n)
grid = expand.grid(X1 = xGrid, X2 = xGrid)
grid = cbind(grid$X1,grid$X2)
decisionSurface = matrix(as.numeric(kdeTreino$classificar(kdeTreino, grid)), nrow = n, ncol = n)
kdeTreino$classificar(kdeTreino, grid))
kdeTreino$classificar(kdeTreino, grid)
List = kdeTreino
dados = grid
i = 1
retClass = list()
retClass$class = c()
retClass$espVero = matrix(ncol = 2)
desidade1 = desidadeKernel(dados[i,], List$classe1)
desidade2 = desidadeKernel(dados[i,], List$classe2)
retClass$espVero[i,1] = desidade1
retClass$espVero[i,2] = desidade2
K = ( (desidade1*List$classe1$priori) / (desidade2*List$classe2$priori) )
if(K >= 1){
retClass$class[i] = List$classe1$level
}else{
retClass$class[i] = List$classe2$level
}
i = 2
desidade1 = desidadeKernel(dados[i,], List$classe1)
desidade2 = desidadeKernel(dados[i,], List$classe2)
retClass$espVero[i,1] = desidade1
densidade1 = c()
densidade2 = c()
for(i in 1:dim(dados)[1]){
desidade1[i] = desidadeKernel(dados[i,], List$classe1)
desidade2[i] = desidadeKernel(dados[i,], List$classe2)
K = ( (desidade1[i]*List$classe1$priori) / (desidade2[i]*List$classe2$priori) )
if(K >= 1){
retClass$class[i] = List$classe1$level
}else{
retClass$class[i] = List$classe2$level
}
}
dim(dados)[1]
}
}
}
densidade1 = c()
densidade2 = c()
dim(dados)[1]
for(i in 1:dim(dados)[1]){
desidade1[i] = desidadeKernel(dados[i,], List$classe1)
desidade2[i] = desidadeKernel(dados[i,], List$classe2)
K = ( (desidade1[i]*List$classe1$priori) / (desidade2[i]*List$classe2$priori) )
if(K >= 1){
retClass$class[i] = List$classe1$level
}else{
retClass$class[i] = List$classe2$level
}
}
}
}
for(i in 1:dim(dados)[1]){
desidade1[i] = desidadeKernel(dados[i,], List$classe1)
desidade2[i] = desidadeKernel(dados[i,], List$classe2)
K = ( (desidade1[i]*List$classe1$priori) / (desidade2[i]*List$classe2$priori) )
if(K >= 1){
retClass$class[i] = List$classe1$level
}else{
retClass$class[i] = List$classe2$level
}
}
cbind(densidade1,densidade2)
cbind(as.matrix(densidade1),as.matrix(densidade2)
)
cbind(t(densidade1),t(densidade2))
as.matrix(densidade1)
(densidade1)
cbind(desidade1,desidade2)
rm(list = ls())
source("kdeClassificator.R")
library(caTools) #Função sample.split
library("mlbench")
library("plot3D")
#Variaveis gerais
cores = c("red", "blue", "green")
#Criando 1000 amostras de treinamento
espiral<-mlbench.spirals(1000,1,0.05)
#Os dados de treinamento gerados plotados
plot(espiral$x, col = cores[espiral$classes],
xlab = "X", ylab = "Y", main = "Dados de treinamento")
#Treinamento modelo
kdeTreino = bayesC(espiral$x, espiral$classes)
#Plota superficie de decisão
n = 100
xGrid = seq(from = -1, to = 1, length = n)
grid = expand.grid(X1 = xGrid, X2 = xGrid)
grid = cbind(grid$X1,grid$X2)
decisionSurface = matrix(as.numeric(kdeTreino$classificar(kdeTreino, grid)), nrow = n, ncol = n)
decisionSurface = matrix(as.numeric(kdeTreino$classificar(kdeTreino, grid)$class), nrow = n, ncol = n)
plot(espiral$x, col = cores[espiral$classes],
xlab = "X", ylab = "Y", main = "Contorno de separação das classes")
contour(xGrid, xGrid, decisionSurface, add = TRUE, drawlabels = FALSE)
res = kdeTreino$classificar(kdeTreino, grid)
plot(res$vero)
ggbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbb7  77 ui
install.packages("ROCR")
createFolds
library("ROCR")
? split()
require(stats); require(graphics)
n <- 10; nn <- 100
g <- factor(round(n * runif(n * nn)))
x <- rnorm(n * nn) + sqrt(as.numeric(g))
xg <- split(x, g)
boxplot(xg, col = "lavender", notch = TRUE, varwidth = TRUE)
sapply(xg, length)
sapply(xg, mean)
? sample.split()
sample.split(espiral$classes, 100)
x = sample.split(espiral$classes, 100)
m = espiral$x[x,]
l = espiral$x[!x,]
split = dim(espiral$x)[1]/nfolds
nfolds = 10
dim(espiral$x)[1]/nfolds
smpl = samlpe.split(espiral$classes, nSplit)
smpl = sample.split(espiral$classes, nSplit)
nSplit = dim(espiral$x)[1]/nfolds
smpl = sample.split(espiral$classes, nSplit)
x = espiral$x[smpl,]
classe = espiral$classes[smpl,]
classe = espiral$classes[smpl]
length(classe)
classes = espiral$classes[smpl]
subsets[1] = list(x,classes)
subsets = list()
subsets[1] = list(x,classes)
subsets[[1]] = list(x,classes)
subsets[[1]] = list(x = espiral$x[smpl,],classes = espiral$classes[smpl])
subsets
subsets = list()
subsets[[1]] = list(x = espiral$x[smpl,],classes = espiral$classes[smpl])
espiral$x = espiral$x[!smpl,]
length(espiral$classes)
espiral$classes = espiral$classes[!smpl]
length(espiral$classes)
i = 2
smpl = sample.split(espiral$classes, nSplit)
subsets[[i]] = list(x = espiral$x[smpl,],classes = espiral$classes[smpl])
espiral$x = espiral$x[!smpl,]
espiral$classes = espiral$classes[!smpl]
